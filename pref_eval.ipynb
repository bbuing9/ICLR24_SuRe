{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preference Evaluation between SuRe's summarization and Generic summarization\n",
    "\n",
    "- Note. We assume that one already run SuRe and obtained the conditional summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset\n",
    "- Available: ['nq-test', 'wq-test', 'hotpotqa', '2wikimultihopqa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = '2wikimultihopqa-bm25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open(f'./datasets/{data_type}.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup OpenAI\n",
    "- Caution. One needs to insert the proper API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"\"\n",
    "model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import api_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(dataset, idx, n_articles=10, start_idx=0):\n",
    "    data = dataset[idx]\n",
    "    len_ctxs = len(data['contexts'])\n",
    "\n",
    "    text = ''\n",
    "    for i in range(start_idx, start_idx + n_articles):\n",
    "        idx_ctx = (i % len_ctxs)\n",
    "        text += f\"\\n\\nPassage #{i+1} Title: {data['contexts'][idx_ctx]['title']}\\nPassage #{i+1} Text: {data['contexts'][idx_ctx]['text']}\"\n",
    "\n",
    "    text += f\"\\n\\nYour job is to act as a professional writer. You will write a good-quality passage that can support the prediction about the question only based on the information in the provided supporting passages.\\n\\nNow, let's start. After you write, please write [DONE] to indicate you are done. Do not write a prefix (e.g., \\\"Response:\\\") while writing a passage.\"\n",
    "    text += f\"\\n\\nQuestion: {data['question']}\"\n",
    "    text += f\"\\nPassage: \"\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_api_summarize(model, dataset, n_articles=10, start_idx=0, temp=0, iters=1):\n",
    "    res = []\n",
    "    \n",
    "    for i, example in tqdm(enumerate(dataset)):\n",
    "        waiting_time = 0.5\n",
    "        query = summarize(dataset, i, n_articles)\n",
    "        answer = api_query(model, query, temp, iters)\n",
    "        res.append(answer)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_summary = use_api_summarize(model, dataset, n_articles=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = 'output_path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./{save_loc}/{data_type}_generic_summary.json', \"w\", encoding='utf-8') as writer:\n",
    "    writer.write(json.dumps(generic_summary, indent=4, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(dataset, idx, summarization):\n",
    "    data = dataset[idx]\n",
    "    summary = summarization[idx]\n",
    "    text = f\"Below is the passage related to the question at the end. After reading the passage, provide correct answer to the question at the end. Answer should not exceed 3 words.\"\n",
    "    text += f\"\\n\\nPassage:\\n{summary}\"\n",
    "    text += f\"\\n\\nQuestion:\\n{data['question']}\"\n",
    "    text += f\"\\n\\nAnswer: \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_api_prediction(model, dataset, summarization, temp=0, iters=1):\n",
    "    res = []\n",
    "    for i, example in tqdm(enumerate(dataset)):\n",
    "        waiting_time = 0.5\n",
    "        query = prediction(dataset, i, summarization)\n",
    "        answer = api_query(model, query, temp, iters)\n",
    "        res.append(answer)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_pred = use_api_prediction(model, dataset, generic_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./{save_loc}/{data_type}_generic_pred.json', \"w\", encoding='utf-8') as writer:\n",
    "    writer.write(json.dumps(generic_pred, indent=4, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import get_em_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_generic, f1_generic = get_em_f1(dataset, generic_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_ans_idx = np.where(em_generic == 1)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SuRe's Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sure_loc = './test_folder/2wiki_start0_end5_sure_ret10/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sure_ans_idx = np.load(sure_loc + 'sure_ans_idx.npy')\n",
    "sure_summary1 = json.load(open(sure_loc + 'summary1.json'))\n",
    "sure_summary2 = json.load(open(sure_loc + 'summary2.json'))\n",
    "all_indices = json.load(open(sure_loc + 'indices.json'))\n",
    "sure_choice1_idx = np.array(all_indices[0])\n",
    "sure_choice2_idx = np.array(all_indices[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain indices of mutually correct predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = []\n",
    "intesrsect = np.intersect1d(generic_ans_idx, sure_ans_idx)\n",
    "for i in intesrsect:\n",
    "    if i in np.concatenate([sure_choice1_idx, sure_choice2_idx]):\n",
    "        test_idx.append(i)\n",
    "print(len(test_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preference1(dataset, idx, naive_reason, ours_reason, ours_idx):\n",
    "    data = dataset[idx]\n",
    "    question = data['question']\n",
    "    answer = data['answers']\n",
    "    naive_reasoning_txt = naive_reason[idx]\n",
    "    ours_reasoning = ours_reason[ours_idx][idx]\n",
    "    \n",
    "    text = \"Question: Given the following summaries for the target question, determine which one is more informative and plausible as rationale to support a given target question-answer pair.\"\n",
    "    text += f\"\\n\\nSummary 1:\\n{naive_reasoning_txt}\"\n",
    "    text += f\"\\n\\nSummary 2:\\n{ours_reasoning}\"\n",
    "    text += f\"\\n\\nTarget Question:\\n{question}\"\n",
    "    text += \"\\n\\nTarget Answer:\\n\"\n",
    "    for j in range(len(answer)):\n",
    "        text += answer[j]\n",
    "        if j != (len(answer)-1):\n",
    "            text += \", \"\n",
    "    text += \"\\n\\nYour Task:\\nIdentify which summary (Summary 1 or Summary 2) is more informative and plausible as rationale to support a given answer at hand. Choices: [Summary 1, Summary 2].\\n\\nAnswer:\"\n",
    "    return text\n",
    "\n",
    "def preference2(dataset, idx, naive_reason, ours_reason, ours_idx):\n",
    "    data = dataset[idx]\n",
    "    question = data['question']\n",
    "    answer = data['answers']\n",
    "    naive_reasoning_txt = naive_reason[idx]\n",
    "    ours_reasoning = ours_reason[ours_idx][idx]\n",
    "    \n",
    "    text = \"Question: Given the following summaries for the target question, determine which one is more informative and plausible as rationale to support a given target question-answer pair.\"\n",
    "    text += f\"\\n\\nSummary 1:\\n{ours_reasoning}\"\n",
    "    text += f\"\\n\\nSummary 2:\\n{naive_reasoning_txt}\"\n",
    "    text += f\"\\n\\nTarget Question:\\n{question}\"\n",
    "    text += f\"\\n\\nTarget Answer:\\n\"\n",
    "    for j in range(len(answer)):\n",
    "        text += answer[j]\n",
    "        if j != (len(answer)-1):\n",
    "            text += \", \"\n",
    "    text += \"\\n\\nYour Task:\\nIdentify which summary (Summary 1 or Summary 2) is more informative and plausible as rationale to support a given answer at hand. Choices: [Summary 1, Summary 2].\\n\\nAnswer:\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_api_preference(model, dataset, test_idx, naive_reason, ours_reason, choice1_idx, choice2_idx, iters=1, temp=0.0):\n",
    "    res1, res2 = [], []\n",
    "    for i in tqdm(range(len(test_idx))):\n",
    "        data_idx = test_idx[i]\n",
    "        if data_idx in choice1_idx:\n",
    "            ours_idx_tmp = 0\n",
    "        elif data_idx in choice2_idx:\n",
    "            ours_idx_tmp = 1\n",
    "        else:\n",
    "            print('Something wrong')\n",
    "            \n",
    "        query1 = preference1(dataset, data_idx, naive_reason, ours_reason, ours_idx_tmp)\n",
    "        query2 = preference2(dataset, data_idx, naive_reason, ours_reason, ours_idx_tmp)\n",
    "        \n",
    "        answer1 = api_query(model, query1, temp, iters)[0]\n",
    "        answer2 = api_query(model, query2, temp, iters)[0]\n",
    "        \n",
    "        res1.append(answer1)\n",
    "        res2.append(answer2)\n",
    "    return res1, res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference1_result, preference2_result = use_api_preference(model, dataset, test_idx, generic_summary, [sure_summary1, sure_summary2], sure_choice1_idx, sure_choice2_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(len(preference1_result)):\n",
    "    result1 = preference1_result[i]\n",
    "    result2 = preference2_result[i]\n",
    "    tmp = np.zeros(3)\n",
    "    r1 = result1[:11]\n",
    "    r2 = result2[:11]\n",
    "    if ('Summary 1' in r1 and 'Summary 2' in r2):\n",
    "        tmp[0] += 1\n",
    "    elif ('Summary 2' in r1 and 'Summary 1' in r2):\n",
    "        tmp[2] += 1\n",
    "    elif ('Summary 1' in r1 and 'Summary 1' in r2):\n",
    "        tmp[1] += 1\n",
    "    elif ('Summary 2' in r1 and 'Summary 2' in r2):\n",
    "        tmp[1] += 1\n",
    "    else:\n",
    "        tmp[1] += 1\n",
    "    result.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lose = 0\n",
    "tie = 0\n",
    "win = 0\n",
    "for x in result:\n",
    "    l = x[0]/sum(x)\n",
    "    t = x[1]/sum(x)\n",
    "    w = x[2]/sum(x)\n",
    "    lose += l\n",
    "    tie += t\n",
    "    win += w\n",
    "print(lose/len(result), tie/len(result), win/len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft",
   "language": "python",
   "name": "peft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cfe34a9389bfb9158f4a57d38254999ecb4846a6b929cd8c17eb23c1b8c530ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
