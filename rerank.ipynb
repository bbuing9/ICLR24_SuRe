{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking Capability of SuRe \n",
    "\n",
    "- Note. We assume that one already run (1) SuRe and obtained the conditional summarization, and (2) Preference Evaluation and obtained the generic summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset\n",
    "- Available: ['nq-test', 'wq-test', 'hotpotqa', '2wikimultihopqa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = '2wikimultihopqa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open(f'./datasets/{data_type}-bm25.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup OpenAI\n",
    "- Caution. One needs to insert the proper API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"\"\n",
    "model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import api_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method #1: Top-1 with BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import use_api_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_retrieval1 = use_api_base(model, dataset, iters=1, n_articles=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import get_em_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_top1, f1_top1 = get_em_f1(dataset, base_retrieval1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EM: {}, F1: {}\".format(em_top1.mean(), f1_top1.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method #2. Similarity with question (using sentence embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sent = SentenceTransformer('msmarco-MiniLM-L6-cos-v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_question(model, examples, n_articles=10):\n",
    "    res = []\n",
    "\n",
    "    for i, example in tqdm(enumerate(examples)):\n",
    "        query_embedding = model.encode(example['question'])\n",
    "        \n",
    "        ctxs = []\n",
    "        for i in range(n_articles):\n",
    "            ctxs.append(example['contexts'][i]['text'])\n",
    "        passage_embedding = model.encode(ctxs)     \n",
    "\n",
    "        cosine_sim = util.dot_score(query_embedding, passage_embedding)\n",
    "        res.append(example['contexts'][int(np.argmax(cosine_sim[0]))])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_sim_q = get_sim_question(model_sent, dataset, n_articles=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_selected(dataset, selected, idx):\n",
    "    data = dataset[idx]\n",
    "    text = \"\"\n",
    "    text += f\"Passage #1 Title: {selected[idx]['title']}\\nPassage #1 Text: {selected[idx]['text']} \\n\\n\"\n",
    "    text += f\"Task description: predict the answer to the following question. Do not exceed 3 words.\"\n",
    "    text += f\"\\n\\nQuestion: {data['question']}.\"\n",
    "    text += f\"\\n\\nAnswer: \"          \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_api_rerank(model, dataset, selected, iters=1, temp=0.0):\n",
    "    res = []\n",
    "    \n",
    "    for i, example in tqdm(enumerate(dataset)):\n",
    "        query = get_pred_selected(dataset, selected, idx=i)\n",
    "        answer = api_query(model, query, temp, iters)\n",
    "        res.append(answer)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sim_q = use_api_rerank(model, dataset, rerank_sim_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sim_q, f1_sim_q = get_em_f1(dataset, pred_sim_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EM: {}, F1: {}\".format(em_sim_q.mean(), f1_sim_q.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method #3: LLM as Reranker\n",
    "- This idea is published in EMNLP23 (\"Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents\")\n",
    "- Codes are adopted from [the official repository](https://github.com/sunnweiwei/RankGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_rerank(examples, idx, n_articles=10, start_idx=0):\n",
    "    data = examples[idx]\n",
    "    len_ctxs = len(data['contexts'])\n",
    "    num = n_articles\n",
    "    query = f\"Question: {data['question']}\"\n",
    "    \n",
    "    text = \"This is RankGPT, an intelligent assistant that can rank passages based on their relevancy to the query.\\n\\n\"\n",
    "    text += f\"The following are {num} passages, each indicated by number identifier []. I can rank them based on their relevance to query: {query}\\n\\n\"\n",
    "    supporting_articles = ''\n",
    "    for i in range(n_articles):\n",
    "        idx_ctx = ((i+start_idx) % len_ctxs)\n",
    "        supporting_articles += f\"[{i+1}] Title: {data['contexts'][idx_ctx]['title']}\\nText: {data['contexts'][idx_ctx]['text']}\\n\\n\"\n",
    "    text += supporting_articles\n",
    "    \n",
    "    text += f\"The search query is: {query}\\n\\n\"\n",
    "    text += f\"I will rank the {num} passages above based on their relevance to the search query. The passages will be listed in descending order using identifiers, and the most relevant passages should be listed first, and the output format should be [] > [] > etc, e.g., [1] > [2] > etc.\\n\\n\"\n",
    "    text += f\"The ranking results of the {num} passages (only identifiers) is:\"\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_output(output, window_size=5):\n",
    "    res_rank = np.zeros(window_size).astype(np.int64)\n",
    "    splitted = output.split(' > ')\n",
    "    if len(splitted) != window_size:\n",
    "        res_rank = np.arange(window_size).astype(np.int64) + 1\n",
    "    else:    \n",
    "        for i in range(window_size):\n",
    "            try: \n",
    "                if splitted[i][2] == '0':\n",
    "                    ele = int(splitted[i][1:3])\n",
    "                else:\n",
    "                    ele = int(splitted[i][1])\n",
    "            except:\n",
    "                print(splitted)\n",
    "                for j in range(window_size):\n",
    "                    if f'[{j+1}]' not in splitted:\n",
    "                        ele = f'{j+1}'\n",
    "                        break\n",
    "            res_rank[i] = int(ele)\n",
    "\n",
    "    return res_rank - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(model, examples, idx, n_articles=10, window_size=5, step_size=3):\n",
    "    data = examples[idx]\n",
    "    n_ctxs = n_articles\n",
    "    init_rank = np.arange(n_ctxs)\n",
    "\n",
    "    for iter in range(n_ctxs // step_size):\n",
    "        if iter == (n_ctxs // step_size) - 1:\n",
    "            start_idx = 0\n",
    "        else:\n",
    "            start_idx = n_ctxs - window_size - iter * step_size\n",
    "        query_rerank = prompt_rerank(examples, idx, window_size, start_idx)\n",
    "        output = api_query(model, query_rerank, temp=0, iters=1)[0]\n",
    "        ordering = parsing_output(output, window_size)\n",
    "        init_rank[start_idx:start_idx+window_size] = init_rank[start_idx:start_idx+window_size][ordering]\n",
    "\n",
    "    return init_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_all_data(model, dataset):\n",
    "    res = []\n",
    "    n_samples = len(dataset)\n",
    "    for i in tqdm(range(n_samples)):\n",
    "        res_i = sliding_window(model, dataset, i)\n",
    "        res.append(res_i)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ranking_window = rerank_all_data(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_llm = []\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    rerank_llm.append(item['contexts'][int(get_ranking_window[i][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_llm = use_api_rerank(model, dataset, rerank_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_llm, f1_llm = get_em_f1(dataset, pred_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EM: {}, F1: {}\".format(em_llm.mean(), f1_llm.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method #4: Similarity with Generic Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_loc = './temp.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_summary = json.load(open(generic_loc))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_summary(model, dataset, summaries, n_articles=10):\n",
    "    res = []\n",
    "\n",
    "    for i, example in tqdm(enumerate(dataset)):\n",
    "        query_embedding = model.encode(summaries[i])\n",
    "        \n",
    "        ctxs = []\n",
    "        for i in range(n_articles):\n",
    "            ctxs.append(example['contexts'][i]['text'])\n",
    "        passage_embedding = model.encode(ctxs)     \n",
    "\n",
    "        # compute and print the cosine similarity matrix\n",
    "        cosine_sim = util.dot_score(query_embedding, passage_embedding)\n",
    "        res.append(example['contexts'][int(np.argmax(cosine_sim[0]))])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_sim_gen_summary = get_sim_summary(model_sent, dataset, generic_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sim_gen_summary = use_api_rerank(model, dataset, rerank_sim_gen_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sim_gen_summary, f1_sim_gen_summary = get_em_f1(dataset, pred_sim_gen_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EM: {}, F1: {}\".format(em_sim_gen_summary.mean(), f1_sim_gen_summary.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method #5: Similarity with SuRe's Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sure_loc = './temp2/results_summary.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sure_summary = json.load(open(sure_loc))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_sim_sure_summary = get_sim_summary(model_sent, dataset, sure_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sim_sure_summary = use_api_rerank(model, dataset, rerank_sim_sure_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sim_sure_summary, f1_sim_sure_summary = get_em_f1(dataset, pred_sim_sure_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EM: {}, F1: {}\".format(em_sim_sure_summary.mean(), f1_sim_sure_summary.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maver",
   "language": "python",
   "name": "naver"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cfe34a9389bfb9158f4a57d38254999ecb4846a6b929cd8c17eb23c1b8c530ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
